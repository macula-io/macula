# v0.9.0 Consistency and Correlation Concerns

**Date:** 2025-11-18
**Status:** Planning / v0.9.0 scope
**Context:** Identified during docker-compose.demo.yml architecture discussion

## Background

With capability-based on-demand connections, peers only connect when they discover a capability they need (via DHT). This lazy connection model creates consistency challenges around:
- Request/response correlation
- Connection state management
- Message ordering and delivery guarantees
- Stale DHT results

## Core Problem

**Peers don't pre-connect after DHT discovery - they connect on-demand when needed:**

```
1. Peer1 needs "game.start" RPC
   â†’ DHT query: "who has game.start?"
   â†’ DHT: "Peer2 at 172.30.0.32:4433"
   â†’ Peer1 connects to Peer2 ONLY NOW
   â†’ Call RPC
   â†’ Reuse connection for future calls

2. No connection to Peer3 until Peer1 needs a capability Peer3 provides
```

This creates timing and consistency challenges.

---

## Consistency Scenarios Requiring Correlation

### 1. Request/Response Correlation (RPC)

**Problem:**
```erlang
% Multiple concurrent RPC calls to same peer
Peer1 â†’ Peer2: call("game.start", [])    % RequestID?
Peer1 â†’ Peer2: call("game.status", [])   % RequestID?

% Responses arrive out of order - which is which?
Peer1 â† Peer2: {response, Result1}
Peer1 â† Peer2: {response, Result2}
```

**Solution needed:**
- Unique correlation ID per request
- Pending calls registry: `#{RequestID => {FromPid, TimeoutRef, Timestamp}}`
- Timeout handling (request never gets response)
- Response matching via RequestID

**Current state:** `macula_rpc_handler` may already have this - needs verification

---

### 2. Connection State Management

**Problem:**
```
Time T1: Peer1 connects to Peer2 for RPC call
Time T2: Connection idle for 5 minutes
Time T3: Peer1 calls RPC again - connection still alive?
Time T4: Peer2 crashed - stale connection reference
```

**Solution needed:**
```erlang
% Connection registry per peer/capability
Connections = #{
  {peer2, "172.30.0.32:4433"} => #{
    conn_pid => Pid,
    capabilities => ["game.start", "game.status"],
    last_used => Timestamp,
    health => alive | stale | dead
  }
}

% Connection lifecycle:
% - Reuse if exists and healthy
% - Reconnect if stale/dead
% - Idle timeout (close after N minutes unused)
% - Keepalive probes (QUIC ping frames?)
```

---

### 3. DHT Result Caching with TTL

**Problem:**
```
Time T1: DHT says Peer2 has "game.start"
Time T2: Peer1 caches result
Time T3: Peer2 crashes and restarts WITHOUT "game.start"
Time T4: Peer1 tries cached result â†’ FAIL
```

**Solution needed:**
```erlang
% Capability cache with TTL
CapabilityCache = #{
  "game.start" => #{
    providers => [{peer2, "172.30.0.32:4433"}, {peer3, "172.30.0.33:4433"}],
    expires_at => Timestamp + 300_000,  % 5 min TTL
    version => 1
  }
}

% Invalidation triggers:
% - TTL expiry
% - Connection failure to provider
% - DHT update notification (if provider stops advertising)
% - Manual refresh request
```

**Trade-offs:**
- Short TTL: More DHT queries (overhead)
- Long TTL: Stale results (failures)
- Recommended: 5 min TTL + invalidate on connection failure

---

### 4. Message Ordering and Sequencing (PubSub)

**Problem:**
```
Peer1 publishes to "game.events":
  T1: {event: "player_joined", player: "Alice"}
  T2: {event: "player_joined", player: "Bob"}
  T3: {event: "game_started"}

Peer2 receives out of order due to network delays:
  T3, T1, T2  â† Wrong order!
```

**Solution needed:**
```erlang
% Per-topic sequence numbers
{publish, TopicID, SeqNum, Message}

% Publisher maintains sequence per topic:
TopicSeq = #{
  "game.events" => 42,
  "player.updates" => 15
}

% Subscriber can:
% - Detect gaps (seq 10, 12 arrived but not 11)
% - Request retransmission
% - Buffer out-of-order messages
% - Or accept best-effort delivery
```

---

### 5. Delivery Guarantees

**Trade-offs for different use cases:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Guarantee          â”‚ Complexity   â”‚ Overhead   â”‚ Use Case     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ At-most-once       â”‚ Low          â”‚ Low        â”‚ Telemetry    â”‚
â”‚ (fire and forget)  â”‚              â”‚            â”‚ Logging      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ At-least-once      â”‚ Medium       â”‚ Medium     â”‚ Events       â”‚
â”‚ (retry + ACK)      â”‚              â”‚            â”‚ Notificationsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Exactly-once       â”‚ High         â”‚ High       â”‚ Transactions â”‚
â”‚ (dedup + ACK)      â”‚              â”‚            â”‚ Payments     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recommendation:**
- Default: At-most-once (Macula v0.8.0)
- Optional: At-least-once (v0.9.0)
- Future: Exactly-once (v1.0.0 if needed)

---

## Specific Inconsistency Scenarios

### A. Split DHT Results
```
Time T1: DHT says Peer2 has "game.start"
Time T2: Peer1 caches result
Time T3: Peer2 crashes
Time T4: Peer1 tries to connect â†’ FAIL
```

**Solution:**
1. Catch connection failure
2. Remove Peer2 from cache
3. Retry DHT query
4. Try next provider if available
5. Return error if no providers

**Pattern:** Circuit breaker + failover

---

### B. Stale Connections
```
Connection idle for 5 minutes
Peer sends message â†’ QUIC says "sent" but peer is dead
No response ever comes back
```

**Solution:**
1. Idle timeout: Close unused connections after N minutes
2. Keepalive: Send QUIC PING frames periodically
3. Connection health: Track last successful message
4. Dead connection detection: Timeout on request + reconnect

**Pattern:** Connection lifecycle management

---

### C. Race Conditions in PubSub
```
Peer1 publishes to "game.events"
DHT says: Peer2, Peer3 subscribed
Peer4 subscribes 1ms later
â†’ Peer4 misses message
```

**Solution options:**

**Option 1: Accept message loss** (simplest)
- Best-effort delivery
- Subscribers responsible for handling gaps

**Option 2: Subscription acknowledgment** (complex)
- Subscribe returns SeqNum of next message
- Publisher buffers recent messages
- New subscribers get buffered messages

**Option 3: Message retention** (most complex)
- Topic-based message log (like Kafka)
- Subscribers can replay from SeqNum
- Requires persistent storage

**Recommendation:** Option 1 for v0.8.0, Option 2 for v0.9.0 if needed

---

## Proposed Solutions for v0.9.0

### RPC Correlation
- âœ… Request correlation IDs (verify if exists in `macula_rpc_handler`)
- âœ… Pending calls registry with timeouts
- âœ… Automatic failover (try next provider on failure)
- âœ… Circuit breaker pattern

### Connection Management
- âœ… Connection pooling per peer
- âœ… Health checks (QUIC keepalive or application-level ping)
- âœ… Idle timeout (close after 5-10 min unused)
- âœ… Reconnect on stale/dead connection

### DHT Caching
- âœ… Result caching with 5 min TTL
- âœ… Invalidate on connection failure
- âœ… Exponential backoff on retries
- âœ… Refresh on cache miss

### PubSub Ordering
- â³ Message sequence numbers per topic (optional)
- â³ Best-effort delivery (accept some loss)
- â³ Subscriber versioning (track subscription changes)
- ğŸš« Message buffering (defer to v1.0.0 if needed)

### General Patterns
- âœ… Circuit breaker (stop trying dead peers)
- âœ… Exponential backoff on retries
- âœ… Connection lifecycle hooks
- âœ… Monitoring/observability (track failures, timeouts)

---

## Implementation Priority for v0.9.0

**Phase 1: Critical (must-have)**
1. Request correlation IDs for RPC
2. Connection health tracking
3. DHT result caching with TTL
4. Timeout handling

**Phase 2: Important (should-have)**
5. Connection pooling and reuse
6. Circuit breaker pattern
7. Automatic failover
8. Exponential backoff

**Phase 3: Nice-to-have (could-have)**
9. PubSub sequence numbers
10. Message buffering
11. Subscription acknowledgment

---

## Related Documents

- `architecture/NAT_TRAVERSAL_ROADMAP.md` - v0.8.0/v0.9.0 connectivity improvements
- `architecture/dht_routed_rpc.md` - DHT-based RPC routing design
- `architecture/pubsub_optimization_recommendations.md` - PubSub throughput improvements

---

## Open Questions

1. **Connection idle timeout**: 5 min? 10 min? Configurable?
2. **DHT cache TTL**: 5 min default? Per-capability override?
3. **Retry strategy**: How many retries? Exponential backoff parameters?
4. **Circuit breaker thresholds**: How many failures before opening circuit?
5. **PubSub guarantees**: Stick with at-most-once or implement at-least-once?
6. **Message ordering**: Enforce strict ordering or allow reordering?

---

## Next Steps

1. Audit existing code for correlation mechanisms (especially `macula_rpc_handler`)
2. Design connection lifecycle state machine
3. Implement DHT result caching (separate module?)
4. Add connection health tracking to `macula_connection`
5. Document consistency guarantees in API docs
6. Add metrics/observability for failures and timeouts

---

**Note:** This document captures architectural concerns for v0.9.0 planning. Implementation details TBD based on v0.8.0 lessons learned and production experience.
